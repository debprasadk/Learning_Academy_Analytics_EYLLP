{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeD8WGmObs_I"
      },
      "source": [
        "# **Hierarchical Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9j_qmmEbzuy"
      },
      "source": [
        "## **Agenda**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOoy8vQqb-f9"
      },
      "source": [
        "In this session, we will cover the following concepts with the help of a business use case:\n",
        "* Hierarchical Clustering\n",
        "* Types of Hierarchical Clustering\n",
        "* Evaluate similarities between clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhaDIF7ATDV5"
      },
      "source": [
        "## <b> Hierarchical Clustering </b>\n",
        "\n",
        "\n",
        "Hierarchical clustering is where we build a cluster tree (dendrogram) to represent data, where each group (node) is linked to two or more successor groups.\n",
        "* The groups are nested and organized as a tree, which ideally ends up as a meaningful classification scheme.\n",
        "* Each node in the cluster tree contains a group of similar data; nodes are placed on the graph next to other similar nodes.\n",
        "* Clusters at one level are joined with clusters in the next level up, using a degree of similarity.\n",
        "* The process carries on until all nodes are in the tree, which gives a visual snapshot of the data contained in the whole set.\n",
        "* The total number of clusters is not predetermined before you start the tree creation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUkITqtYUZhC"
      },
      "source": [
        "![clustergram](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.9_Unsupervised_Learning/Trainer_PPT_and_IPYNB/0.3_Hierarchical_Clustering/clustergram.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHKNmIJLUvW-"
      },
      "source": [
        "## <b> Implementing Hierarchical Clustering </b>\n",
        "\n",
        "\n",
        "There are two major ways in which hierarchical clustering can be carried out:\n",
        "\n",
        "1. Agglomerative or Bottom-Up Clustering\n",
        "2. Divisive or Top-Down Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhyhEjwkH8pl"
      },
      "source": [
        "### <b> Agglomerative (Bottom-Up) Clustering </b>\n",
        "\n",
        "1. Start with each example in its own singleton cluster\n",
        "2. At each time-step, greedily merge two most similar clusters\n",
        "3. Stop when there is a single cluster of all examples, else go to two"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP1VqVoCHlTr"
      },
      "source": [
        "### <b> Divisive (Top-Down) Clustering </b>\n",
        "\n",
        "\n",
        "1. Start with all examples in the same cluster\n",
        "2. At each time-step, remove the “outsiders” from the least cohesive cluster\n",
        "3. Stop when each example is in its own singleton cluster, else go to two"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgvSkwfOHvhB"
      },
      "source": [
        "### <b> Which One to Use? </b>\n",
        "\n",
        "* Agglomerative Clustering is simpler than the divisive clustering to implement because for the latter, we need a second, flat clustering algorithm as a `subroutine`.\n",
        "* However, top-down routine has the advantage of being more efficient if we do not generate a complete hierarchy all the way down to individual document leaves.\n",
        "* For a fixed number of top levels, using an efficient flat algorithm like  $K$-means. Top-down algorithms are linear in the number of documents and clusters.\n",
        "* There is also evidence that divisive algorithms produce more accurate hierarchies than bottom-up algorithms. In some circumstances according to the [Stanford University Review](https://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-17.html#sec:hclstfurther)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK1lfrXfIOvs"
      },
      "source": [
        "### <b> Deciding (Dis)similarity between Clusters </b>\n",
        "\n",
        "In both techniques, we discussed finding the similarity or dissmilarity between the two clusters. The question is, \"How to measure them?\"\n",
        "\n",
        "\n",
        "Before any clustering is performed, it is required that you determine the **proximity matrix containing the distance between each point using a distance function**.\n",
        "Then, the matrix is updated to display the distance between each cluster.\n",
        "The following three methods differ in how the distance between each cluster is measured."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9wdKENK5f3q"
      },
      "source": [
        "### <b> Dendrograms </b>\n",
        "\n",
        "We can use a dendrogram to visualize the history of groupings and figure out the optimal number of clusters.\n",
        " - Determine the largest vertical distance that doesn’t intersect any of the other clusters\n",
        " - Draw a horizontal line at both extremities\n",
        " - The optimal number of clusters is equal to the number of vertical lines going through the horizontal line\n",
        "\n",
        "For example, in the below case, best choice for no. of clusters will be 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiwGcdSt6k2K"
      },
      "source": [
        "![image_1](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.9_Unsupervised_Learning/Trainer_PPT_and_IPYNB/0.3_Hierarchical_Clustering/image_1.JPG)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQINoDiQ58q9"
      },
      "source": [
        "## <b> Linkage Criteria </b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-oDXXnfIUVA"
      },
      "source": [
        "### <b> Single Linkage </b>\n",
        "\n",
        "In single linkage hierarchical clustering, the distance between two clusters is defined as the shortest distance between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the arrow between their two closest points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-xJqtKR1Eod"
      },
      "source": [
        "![image05](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.9_Unsupervised_Learning/Trainer_PPT_and_IPYNB/0.3_Hierarchical_Clustering/image05.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KTiDcyPIf42"
      },
      "source": [
        "### <b> Complete Linkage </b>\n",
        "\n",
        "In complete linkage hierarchical clustering, the distance between two clusters is defined as the longest distance between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the arrow between their two furthest points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Fv-hxYK1ImV"
      },
      "source": [
        "![image06](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.9_Unsupervised_Learning/Trainer_PPT_and_IPYNB/0.3_Hierarchical_Clustering/image06.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onsIeIGlInG7"
      },
      "source": [
        "### <b> Average Linkage </b>\n",
        "\n",
        "In average linkage hierarchical clustering, the distance between two clusters is defined as the average distance between each point in one cluster to every point in the other cluster. For example, the distance between clusters “r” and “s” to the left is equal to the average length of each arrow between connecting the points of one cluster to the other.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFEz3ZyO1Ki9"
      },
      "source": [
        "![image07](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.9_Unsupervised_Learning/Trainer_PPT_and_IPYNB/0.3_Hierarchical_Clustering/image07.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7ESvhz99Fq2"
      },
      "source": [
        "### <b> Ward Linkage </b>\n",
        "In ward linkage hierarchical clustering, the distance between clusters is the sum of squared differences within all clusters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7TVoTneJ78u"
      },
      "source": [
        "### <b> Single vs. Complete vs. Average Linkage: </b>\n",
        "\n",
        "* Single\n",
        "linkage sometimes produces chaining amongst the clusters. This means that several clusters may be joined together simply because one of their\n",
        "cases is within close proximity of the case from a separate\n",
        "cluster. This problem is specific to single linkage due to\n",
        "the fact that the smallest distance between pairs is the\n",
        "only value taken into consideration.\n",
        "* In\n",
        "complete linkage, outlying cases prevent close clusters\n",
        "to merge together because the measure of the furthest\n",
        "neighbor exacerbates the effects of outlying data.\n",
        "* Average linkage is supposed to represent a\n",
        "natural compromise between the linkage measures to\n",
        "provide a more accurate evaluation of the distance\n",
        "between the clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjMZKbm38Y8g"
      },
      "source": [
        "## <b> Distance Metric </b>\n",
        "The method you use to calculate the distance between data points will affect the end result.\n",
        "### <b>Euclidean Distance </b>\n",
        "The shortest distance between two points. <br>\n",
        "For example,\n",
        "if x=(a,b) and y=(c,d), <br>\n",
        "the Euclidean distance between x and y is √(a−c)²+(b−d)²\n",
        "### <b> Manhattan Distance </b>\n",
        "Imagine you were in the downtown center of a big city and you wanted to get from point A to point B. You wouldn’t be able to cut across buildings, rather you’d have to make your way by walking along the various streets. <br>\n",
        "For example, if x=(a,b) and y=(c,d), <br>\n",
        "the Manhattan distance between x and y is |a−c|+|b−d|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE7ySio-KKIs"
      },
      "source": [
        "## <b> Shortcomings </b>\n",
        "\n",
        "One of the biggest drawbacks of hierarchical clustering is it is extremely calculation heavy. Therefore, they are not scalable. That also means that they are not very useful for larger datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSKFQ8vPz96R"
      },
      "source": [
        "`Scipy` has a really convenient api for carrying out hierarchical clustering. Let's see how it works. We will start with necessary imports:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVrlcRzMZGjf"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dxRQp79cxa0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import load_iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyhPlzM_ZGjg"
      },
      "source": [
        "In the above code, we are using the sklearn library, which contains a lot of tools for machine learning and statistical modeling, including classification, regression, clustering, and dimensionality reduction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDOH73u_c0KI"
      },
      "outputs": [],
      "source": [
        "iris = load_iris()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GV9uelYZGjh"
      },
      "source": [
        "Load_iris is a function from sklearn that loads and returns the iris dataset. The libraries used above already contains it. Just by loading the library, a data frame named \"iris\" will be made available and can be used straight away."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORrHsxS3c810"
      },
      "outputs": [],
      "source": [
        "X = iris.data\n",
        "y = iris.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y98KjZ5RcgDR"
      },
      "outputs": [],
      "source": [
        "#Import dendrogram and linkage module from scipy library\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMm-LBwscgrM"
      },
      "outputs": [],
      "source": [
        "#Generate the linkage matrix\n",
        "Z = linkage(X, 'average')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEr0yqxMck0p"
      },
      "outputs": [],
      "source": [
        "#Calculate full dendrogram\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(25, 10))\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('sample index')\n",
        "plt.ylabel('distance')\n",
        "dendrogram(\n",
        "    Z,\n",
        "    leaf_rotation=90.,  #Rotates the x axis labels\n",
        "    leaf_font_size=8.,  #Font size for the x axis labels\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfzPoRnE2SH6"
      },
      "source": [
        "Now we are going to see the concept of Agglomerative (Bottom-Up) Clustering with a business use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkWrWGps2dqM"
      },
      "source": [
        "### <b> Problem Statement: </b>\n",
        "An ecommerce company has prepared a rough dataset containing shopping details of their customers, which includes CustomerID, Genre, Age, Annual Income (k$), Spending Score (1-100). The company is unable to target a specific set of customers with a particular set of SKUs.\n",
        "\n",
        "### <b> Objective:</b>\n",
        "\n",
        " Segment customers into different groups based on their shopping trends."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akkrEKfSzH9m"
      },
      "source": [
        "### **Dataset**\n",
        "\n",
        "Before reading the data from a .csv file, you need to download \"housing_data.csv\" dataset from the course resource and upload it into the lab.\n",
        "We must use the Up arrow icon, which is shown in the left side under View icon. Click on the Up arrow icon and upload the file\n",
        "wherever it is downloaded into your system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVLDwO7c2dDX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ry7u3oVP3J-p"
      },
      "outputs": [],
      "source": [
        "customer_data = pd.read_csv('shopping_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeR6Tt9SZGjj"
      },
      "source": [
        "In the above code, pd.read_csv function is used to read the \"shopping_data.csv\" file, and customer_data is a variable that will store the data read by the .csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DsbF1H_3g0x"
      },
      "outputs": [],
      "source": [
        "customer_data.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iVH0XW6ZGjk"
      },
      "source": [
        "Here, shape is an attribute that returns a tuple representing the dimensionality of the customer_data. It is used to define the number of rows and columns in customer_data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLXaJQRt3jgw"
      },
      "outputs": [],
      "source": [
        "customer_data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73fHlQwZ3x2x"
      },
      "source": [
        "Our dataset has five columns: CustomerID, Genre, Age, Annual Income, and Spending Score. To view the results in two-dimensional feature space, we will retain only two of these five columns. We can remove CustomerID column, Genre, and Age column. We will retain the Annual Income (in thousands of dollars) and Spending Score (1-100) columns. The Spending Score column signifies how often a person spends money in a mall on a scale of 1 to 100 with 100 being the highest spender. Execute the following script to filter the first three columns from our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90Xh7rKd3mA6"
      },
      "outputs": [],
      "source": [
        "data = customer_data.iloc[:, 3:5].values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "xaxgbGx8jDaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ew7qiwI33Vb"
      },
      "source": [
        "Next, we need to know the clusters that we want our data to be split to. We will again use the scipy library to create the dendrograms for our dataset. Execute the following script to do so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5K9mwat3om5"
      },
      "outputs": [],
      "source": [
        "import scipy.cluster.hierarchy as shc\n",
        "\n",
        "plt.figure(figsize=(25, 10))\n",
        "plt.title(\"Customer Dendograms\")\n",
        "dend = shc.dendrogram(shc.linkage(data, method='ward'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoSDdw9R69jk"
      },
      "source": [
        "If we draw a horizontal line that passes through longest distance without a horizontal line, we get 5 clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoMmAWFV4iWp"
      },
      "source": [
        "We create an instance of Agglomerative Clustering using the euclidean distance as the measure of distance between points and ward linkage to calculate the proximity of clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_33vFwJ4nC8"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\n",
        "cluster.fit_predict(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCxtwDEp4q5h"
      },
      "source": [
        "You can see the cluster labels from all of your data points. Since we had five clusters, we have five labels in the output i.e. 0 to 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyn2JWLp4uSW"
      },
      "source": [
        "As a final step, let's plot the clusters to see how actually our data has been clustered:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2Kjc8PH4nZw"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(data[:,0], data[:,1], c=cluster.labels_, cmap='rainbow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ougs4hrsUSzv"
      },
      "source": [
        "### **Conclusion**\n",
        "When the shopping data is grouped using the agglomerative clustering technique, we can observe that there are five groups for consumers whose labels range from 0 to 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BVrKLeickQN"
      },
      "source": [
        "**Note: In this lesson, we saw the use of the unservised learning methods, but in the next lesson we will be working on \"Time-Series Modelling\".**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH49_luCpp4k"
      },
      "source": [
        "![Simplilearn_Logo](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Logo_Powered_By_Simplilearn/SL_Logo_1.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 [3.10]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}